# AI_Prog4

## Method

### Decision Tree

1. **Max depth**: The maximum depth of the tree, which limits the depth a decision tree can go. Underfitting when too shallow while overfitting when too deep.
2. **Criterion**: The function to measure the quality of a split, can be gini impurity or entropy, which may cause different performance.
3. **Feature importance** (or attribute importance): The way to measure the importance of each feature when splitting datas.

> Feature importance (FI) = Feature metric \* number of instances – its left child node metric \* number of instances for the left child – its right child node metric \* number of instances for the right child

### Random Forest

In random forest, there are two randomness. One is **random subfeatures**, the other is **random subsamples**.

1. **Max features** (or max attributes): The number of random select features to consider when looking for the best split. Appropriate feature number may improve the robustness of the model and decrease the computational cost.
2. **Bootstrap**: Each tree uses resmpling data from original dataset with-replacement with same size as original.

Aggregating each tree and use majority vote for the predict makes model more robust.

## Experiment and Conclusion

**Dataset**: [Glass Identification Data Set](https://archive.ics.uci.edu/ml/datasets/glass+identification) and [Ionosphere Data Set](https://archive.ics.uci.edu/ml/datasets/Ionosphere).  
Abstract:  

|             | Glass | Ionosphere |
| ----------- | ----- | ---------- |
| #instances  | 214   | 351        |
| #attributes | 9     | 34         |
| #classes    | 6     | 2          |

Using k-fold validation, k = 5.

Tables below shows the average accuracy using k-fold validation in different parameter value.

1. **Max depth**

   |      | Glass | Ionosphere |
   | ---- | ----- | ---------- |
   | 3    | 0.663 | 0.900      |
   | 5    | 0.687 | 0.897      |

   A tree with a max depth of 3 can have a good accuracy. Also, shallow tree may have less computation cost.  
   Therefore, the following experiments use Decision Tree with max depth of 3.

2. **Criterion**

   |               | Glass | Ionosphere |
   | ------------- | ----- | ---------- |
   | Gini impurity | 0.668 | 0.886      |
   | Entropy       | 0.327 | 0.898      |

   Gini impurity performs well in both Glass and Ionosphere dataset.  
   Therefore, the following experiments use Decision Tree with gini impurity.

3. **Importance**

   The feature importance of my Decision Tree model without normalize the importance and only list those features with nonzero importance.

   |      | Glass                                                        | Ionosphere                                                   |
   | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
   | FI   | ![image-20200705233453145](C:\Users\berzr\AppData\Roaming\Typora\typora-user-images\image-20200705233453145_cr.png) | ![image-20200705231525297](C:\Users\berzr\AppData\Roaming\Typora\typora-user-images\image-20200705231525297_cr.png) |

   

4. **#attributes**

   |                          | Glass | Ionosphere |
   | ------------------------ | ----- | ---------- |
   | #attributes              | 0.668 | 0.897      |
   | floor(sqrt(#attributes)) | 0.672 | 0.880      |
   | 1                        | 0.603 | 0.849      |

   Using more attributes does not necessarily result in higher accuracy, `floor(sqrt(#attributes))` is enough to train the model, and the model may be more robust, also, a lot more fast.

5. **#estimators**

   |      | Glass | Ionosphere |
   | ---- | ----- | ---------- |
   | 10   | 0.613 | 0.915      |
   | 100  | 0.673 | 0.926      |

   The more the estimators, the higher the accuracy. This is reasonable because Random Forest uses a majority vote to determine the prediction.

6. **acc_oob vs acc_val**

   |         | Glass | Ionosphere |
   | ------- | ----- | ---------- |
   | acc_val | 0.631 | 0.916      |
   | acc_oob | 0.710 | 0.908      |

   Since each estimator in Random Forest only uses the dataset resampled from the original dataset, each of them does not contain some parts of the original dataset, and can be used to predict the accuracy of the entire model.  
   From the result we can find that acc_val and acc_oob are not much different, echoing the theory. And if use out-of-bag accuracy, we don't need to do data validation.

## Appendix

```python
# -*- coding: utf-8 -*-
"""AI_Prog4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16j5ByeYGtBW5hptL_Z08JSx73y6xO4BL

## Method

Some data type which would be used during experiment.

### Data Loader
"""

import pandas as pd

#PATH = 'https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data' # (214, 9, 6)
PATH = 'https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data' # (351, 34, 2)
df = pd.read_csv(PATH, header=None)
df.head()

dataset = df.to_numpy()
print(dataset.shape)

import numpy as np

classes, code = np.unique(dataset[:, -1], return_inverse=True)
dataset[:, -1] = code
print(classes.shape)

x_train = dataset[:, 0:-1] # ionosphere
#x_train = dataset[:, 1:-1] # glass
y_train = dataset[:, -1]
print(x_train.shape)
print(y_train.shape)

"""### Cross Validation"""

def cross_validation(x_train, y_train, k=5):
    kfold_data = list()
    n_samples, n_splits = x_train.shape[0], k

    folds = list()
    perm = np.random.permutation(n_samples)
    idx = 0
    for i in range(n_splits):
        if i < n_samples % n_splits:
            fold_size = n_samples // n_splits + 1
        else:
            fold_size = n_samples // n_splits

        folds.append(list(perm[idx:idx+fold_size]))
        idx += fold_size

    for i in range(n_splits):
        train_folds_idx = list(range(0, i)) + list(range(i+1, n_splits))
        train_idx = list()
        for j in train_folds_idx:
            train_idx += folds[j]
        kfold_data.append([train_idx, folds[i]])

    return kfold_data

kfold_data = cross_validation(np.arange(8), np.arange(8))
print(kfold_data)

"""### CART"""

# Criterion function

def gini(sequence):
    if len(sequence) == 0:
        return 0
    targets, counts = np.unique(sequence, return_counts=True)
    p_counts = counts / counts.sum()
    impure = 1 - np.dot(p_counts, p_counts)
    return impure

def entropy(sequence):
    if len(sequence) == 0:
        return 0
    targets, counts = np.unique(sequence, return_counts=True)
    p_counts = counts / counts.sum()
    impure = -np.dot(p_counts, np.log2(p_counts))
    return impure

class Node():
    def __init__(self, rows, depth):
        self.rows = rows
        self.depth = depth
        self.best_attribute = None
        self.best_threshold = None
        self.left = None
        self.right = None
        self.target = None
        self.impure = None

rng = np.random.default_rng()

class DecisionTree():
    def __init__(self, criterion, max_depth,
                 nun_attributes=None):
        self.criterion = criterion
        self.max_depth = max_depth
        self.root = None
        self.x_train = None
        self.y_train = None
        self.max_attributes = nun_attributes

    def fit(self, x_train, y_train):
        self.root = Node(np.arange(x_train.shape[0]), 0)
        self.x_train = x_train
        self.y_train = y_train
        self.split(self.root)

    def split(self, node):
        node.impure = self.criterion(self.y_train[node.rows])
        attributes = np.arange(self.x_train.shape[1])
        if self.max_attributes != None:
            if self.max_attributes > len(attributes):
                self.max_attributes = len(attributes)
            attributes = rng.choice(attributes, self.max_attributes,
                                    replace=False)
            attributes = np.unique(attributes)

        best_attribute = None
        best_threshold = None
        best_left = None
        best_right = None
        min_impure = 1

        for attribute in attributes:
            for row in node.rows:
                left = list()
                right = list()
                threshold = self.x_train[row][attribute]
                impure = 0

                for i in node.rows:
                    if self.x_train[i][attribute] < threshold:
                        left.append(i)
                    else:
                        right.append(i)
                
                impure += len(left) * self.criterion([self.y_train[j] for j in left])
                impure += len(right) * self.criterion([self.y_train[j] for j in right])
                impure /= len(left) + len(right)

                if impure < min_impure and left and right:
                    best_attribute = attribute
                    best_threshold = threshold
                    best_left = left
                    best_right = right
                    min_impure = impure

        if best_left and best_right and node.depth < self.max_depth:
            node.best_attribute = best_attribute
            node.best_threshold = best_threshold
            node.left = Node(best_left, node.depth + 1)
            node.right = Node(best_right, node.depth + 1)
            self.split(node.left)
            self.split(node.right)
        else:
            unique, counts = np.unique(self.y_train[node.rows], return_counts=True)
            node.target = unique[np.argmax(counts)]
            return

    def predict(self, x_test):
        y_pred = list()

        for x in x_test:
            current = self.root
            while current.target == None:
                if x[current.best_attribute] < current.best_threshold:
                    current = current.left
                else:
                    current = current.right
            y_pred.append(current.target)
    
        return y_pred

    def attribute_importance(self):
        importance = [0] * self.x_train.shape[1]
        q = list()
        q.append(self.root)
        while q:
            node = q.pop()
            if node.best_attribute != None:
                importance[node.best_attribute] += len(node.rows) * node.impure
                importance[node.best_attribute] -= len(node.left.rows) * node.left.impure
                importance[node.best_attribute] -= len(node.right.rows) * node.right.impure
                q.append(node.left)
                q.append(node.right)
        return importance

def accuracy_score(y_pred, y_test):
    return (y_pred == y_test).sum().item() / len(y_pred)

tree = DecisionTree(gini, 3)
tree.fit(x_train, y_train)

y_pred = tree.predict(x_train)
print(accuracy_score(y_pred, y_train))

"""### Random Forest"""

class RandomForest():
    def __init__(self, criterion, max_depth, num_trees,
                 max_attributes=None, bootstrap=True):
        self.trees = list()
        for i in range(num_trees):
            self.trees.append(
                DecisionTree(criterion, max_depth, max_attributes)
            )
        self.bootstrap = bootstrap

    def fit(self, x_train, y_train):
        self.out_of_bag = list()
        for i, tree in enumerate(self.trees):
            print('Fitting tree #%d' % (i+1))
            data_idx = np.arange(x_train.shape[0])
            if self.bootstrap == True:
                data_idx = rng.choice(data_idx, len(data_idx))
                self.out_of_bag.append(
                    np.setxor1d(data_idx, np.arange(x_train.shape[0]))
                )

            _x_train = x_train[data_idx]
            _y_train = y_train[data_idx]
            tree.fit(_x_train, _y_train)
        print('Finished Fitting')
    
    def predict(self, x_test):
        result = list()
        y_pred = list()
        for tree in self.trees:
            y_pred.append(tree.predict(x_test))
        y_pred = np.asarray(y_pred)
        for i in range(y_pred.shape[1]):
            unique, counts = np.unique(y_pred[:,i], return_counts=True)
            result.append(unique[np.argmax(counts)])
        return result

    def accuracy_out_of_bag(self):
        result = list()
        for idx, tree in enumerate(self.trees):
            data_idx = self.out_of_bag[idx]
            x_test = tree.x_train[data_idx]
            y_test = tree.y_train[data_idx]
            y_pred = tree.predict(x_test)
            result.append(accuracy_score(y_pred, y_test))
        return result

forest = RandomForest(gini, 3, 10,
                      max_attributes=int(np.sqrt(x_train.shape[1])))

forest.fit(x_train, y_train)
y_pred = forest.predict(x_train)
accuracy_score(y_pred, y_train)

"""## Experiment

1. Tree depth
"""

def compare(model_list):
    k = 5
    result = np.empty((len(model_list), k))
    kfold_data = cross_validation(x_train, y_train, k=k)
    for i, data_idx in enumerate(kfold_data):
        print('Fold #%d' % (i+1))
        train_idx, test_idx = data_idx
        _x_train, _y_train = x_train[train_idx], y_train[train_idx]
        _x_test, _y_test = x_train[test_idx], y_train[test_idx]

        for j, model in enumerate(model_list):
            model.fit(_x_train, _y_train)
            y_pred = model.predict(_x_test)
            result[j][i] = accuracy_score(y_pred, _y_test)

    return result.mean(axis=1)

clf_depth3 = DecisionTree(criterion=gini, max_depth=3)
clf_depth5 = DecisionTree(criterion=gini, max_depth=5)

comparison = compare([clf_depth3, clf_depth5])
print(comparison)

print('max_depth=3:', comparison[0])
print('max_depth=5:', comparison[1])

"""2. Criterion"""

clf_gini = DecisionTree(criterion=gini, max_depth=3)
clf_entropy = DecisionTree(criterion=entropy, max_depth=3)

comparison = compare([clf_gini, clf_entropy])
print(comparison)

print('Gini:', comparison[0])
print('Entropy:', comparison[1])

"""5. Number of tree"""

clf_10tree = RandomForest(criterion=gini, max_depth=3, num_trees=10,
                          max_attributes=int(np.sqrt(x_train.shape[1])))
clf_100tree = RandomForest(criterion=gini, max_depth=3, num_trees=100,
                           max_attributes=int(np.sqrt(x_train.shape[1])))

comparison = compare([clf_10tree, clf_100tree])
print(comparison)

print('N_estimators=10:', comparison[0])
print('N_estimators=100:', comparison[1])

"""4. Number of attributes"""

clf_na = RandomForest(criterion=gini, max_depth=3, num_trees=10,
                      max_attributes=x_train.shape[1])
clf_ra = RandomForest(criterion=gini, max_depth=3, num_trees=10,
                      max_attributes=int(np.sqrt(x_train.shape[1])))
clf_sa = RandomForest(criterion=gini, max_depth=3, num_trees=10,
                      max_attributes=1)

comparison = compare([clf_na, clf_ra, clf_sa])
print(comparison)

print('max_attributes=n_attributes:', comparison[0])
print('max_attributes=sqrt(n_attributes)', comparison[1])
print('max_attributes=1', comparison[2])

"""6. Out-of-bag error vs validation error"""

clf_acc = RandomForest(criterion=gini, max_depth=3, num_trees=10,
                       max_attributes=int(np.sqrt(x_train.shape[1])))

clf_acc.fit(x_train, y_train)
acc_oob = clf_acc.accuracy_out_of_bag()

print(acc_oob)

k = 5
accs_validation = list()
accs_oob = list()
kfold_data = cross_validation(x_train, y_train, k=k)
for i, data_idx in enumerate(kfold_data):
    train_idx, test_idx = data_idx
    _x_train, _y_train = x_train[train_idx], y_train[train_idx]
    _x_test, _y_test = x_train[test_idx], y_train[test_idx]

    clf_acc.fit(_x_train, _y_train)
    y_pred = clf_acc.predict(_x_test)
    accs_validation.append(accuracy_score(y_pred, _y_test))
    accs_oob.append(np.mean(clf_acc.accuracy_out_of_bag()))

print(accs_validation)
print(accs_oob)

print(np.mean(accs_validation))
print(np.mean(accs_oob))

"""3. Attribute importance"""

clf = DecisionTree(criterion=gini, max_depth=3)

clf.fit(x_train, y_train)
importance = clf.attribute_importance()
print(importance)

import matplotlib.pyplot as plt

importance = np.asarray(importance)
y_pos = np.nonzero(importance)[0]
y_value = importance[y_pos]
fig, ax = plt.subplots()
ax.barh(np.arange(y_pos.shape[0]), y_value)
ax.set_yticks(np.arange(y_pos.shape[0]))
yticklabels = np.asarray(['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe']) # glass
#yticklabels = y_pos # ionosphere
ax.set_yticklabels(yticklabels[y_pos])
plt.show()

"""## Others"""

a = np.array(
    [
        [0, 0, 0, 1, 2, 2, 0, 2],
        [1, 0, 0, 2, 2, 1, 2, 0],
        [1, 0, 0, 1, 1, 1, 1, 2]
    ]
)
b = list()
for i in range(a.shape[1]):
    u, c = np.unique(a[:,i], return_counts=True)
    b.append(u[np.argmax(c)])
print(b)


```

